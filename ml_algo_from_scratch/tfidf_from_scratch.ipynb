{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO65FBrDf4aK1wXZID3Ar4a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/srilamaiti.github.io/blob/main/ml_algo_from_scratch/tfidf_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I44_pEU-10_h",
        "outputId": "e97b9175-c0c4-4ffa-8f61-d7018eff90bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import itertools\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TF_IDF:\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "        self.tf_idf_model = []\n",
        "\n",
        "    def preprocess(self, document): # Add document as a parameter\n",
        "        tokens = word_tokenize(document) # Tokenize individual document\n",
        "        tokens = [token.lower() for token in tokens if token.isalpha()]\n",
        "\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "        '''\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        '''\n",
        "        return tokens # Return preprocessed tokens\n",
        "\n",
        "    def get_tf_idf(self, word, document):\n",
        "        return document.count(word) / len(document)\n",
        "\n",
        "    def get_idf(self, word):\n",
        "        doc_freq = sum(1 for document in self.preprocessed_documents if word in document)\n",
        "        return math.log((len(self.preprocessed_documents) + 1) / ( 1 + doc_freq)) + 1\n",
        "\n",
        "    def get_tf_idf_list(self):\n",
        "        self.preprocessed_documents = [self.preprocess(document) for document in self.documents] # Preprocess each document\n",
        "        self.unique_tokens = list(set(list(itertools.chain(*self.preprocessed_documents))))\n",
        "        for token in self.unique_tokens:\n",
        "            for idx, document in enumerate(self.preprocessed_documents):\n",
        "                tf = self.get_tf_idf(token, document)\n",
        "                idf = self.get_idf(token)\n",
        "                #print(token, idx, tf, idf)\n",
        "                #print(\"***************************\")\n",
        "                tf_idf = tf * idf\n",
        "                self.tf_idf_model.append((token, idx, tf_idf))\n",
        "\n",
        "        return self.tf_idf_model # Return the model outside the loop to get the complete model"
      ],
      "metadata": {
        "id": "ODfr5utRFCIn"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"this is a sample\",\n",
        "    \"this is another example\",\n",
        "    \"this example is different\"\n",
        "]"
      ],
      "metadata": {
        "id": "mSfcUt5IAd_X"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TF_IDF(documents=documents)\n",
        "tfidf.get_tf_idf_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fuB_pQB2gpr",
        "outputId": "22d82a0f-d619-4717-e7e8-2b6b50500273"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('different', 0, 0.0),\n",
              " ('different', 1, 0.0),\n",
              " ('different', 2, 0.8465735902799727),\n",
              " ('sample', 0, 1.6931471805599454),\n",
              " ('sample', 1, 0.0),\n",
              " ('sample', 2, 0.0),\n",
              " ('another', 0, 0.0),\n",
              " ('another', 1, 0.8465735902799727),\n",
              " ('another', 2, 0.0),\n",
              " ('example', 0, 0.0),\n",
              " ('example', 1, 0.6438410362258904),\n",
              " ('example', 2, 0.6438410362258904)]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "# Fit the vectorizer to the documents and transform them into a TF-IDF matrix\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Print the resulting TF-IDF matrix\n",
        "print(tfidf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vLemL6J2j0i",
        "outputId": "b62f5703-b4bc-41b1-a335-ed2dad35636e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 5)\t0.4532946552278861\n",
            "  (0, 3)\t0.4532946552278861\n",
            "  (0, 4)\t0.7674945674619879\n",
            "  (1, 5)\t0.39148397136265967\n",
            "  (1, 3)\t0.39148397136265967\n",
            "  (1, 0)\t0.6628399823470976\n",
            "  (1, 2)\t0.5041068915759233\n",
            "  (2, 5)\t0.39148397136265967\n",
            "  (2, 3)\t0.39148397136265967\n",
            "  (2, 2)\t0.5041068915759233\n",
            "  (2, 1)\t0.6628399823470976\n"
          ]
        }
      ]
    }
  ]
}