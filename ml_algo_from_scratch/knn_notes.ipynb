{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAXuucrTr7avHDY8maBLhD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/srilamaiti.github.io/blob/main/ml_algo_from_scratch/knn_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors (KNN) is a supervised learning algorithm used for classification and regression. It makes predictions based on the K closest data points in the training dataset.\n",
        "\n",
        "üîπ Steps to Implement KNN\n",
        "\n",
        "1Ô∏è‚É£ Store the training data\n",
        "\n",
        "Collect the labeled data (features & target values).\n",
        "\n",
        "2Ô∏è‚É£ Choose the value of K\n",
        "\n",
        "Decide how many nearest neighbors (K) to consider.\n",
        "A small K ‚Üí more sensitive to noise.\n",
        "A large K ‚Üí smoother predictions.\n",
        "\n",
        "3Ô∏è‚É£ Measure the distance between the test point and all training points\n",
        "\n",
        "Common distance metrics:\n",
        "Euclidean distance (most used):\n",
        "ùëë\n",
        "=\n",
        "‚àë\n",
        "(\n",
        "ùë•\n",
        "2\n",
        "‚àí\n",
        "ùë•\n",
        "1\n",
        ")\n",
        "2\n",
        "d=\n",
        "‚àë(x\n",
        "2\n",
        "‚Äã\n",
        " ‚àíx\n",
        "1\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "\n",
        "‚Äã\n",
        " Manhattan distance, Cosine similarity, etc can be used as well.\n",
        "\n",
        "4Ô∏è‚É£ Find the K nearest neighbors\n",
        "\n",
        "Sort the distances and select the K closest points.\n",
        "\n",
        "5Ô∏è‚É£ Make a prediction\n",
        "\n",
        "For Classification: Use majority voting among neighbors.\n",
        "For Regression: Use the average (or weighted average) of neighbors' values.\n",
        "\n",
        "6Ô∏è‚É£ Return the predicted value\n",
        "\n",
        "The final predicted label or numerical value is assigned.\n"
      ],
      "metadata": {
        "id": "DVDRcb41vT2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Advantages of KNN\n",
        "\n",
        "1Ô∏è‚É£ Simple & Easy to Implement\n",
        "\n",
        "No complex mathematical calculations or model training needed.\n",
        "\n",
        "2Ô∏è‚É£ Non-Parametric\n",
        "\n",
        "No assumption about the data distribution (works well with non-linear relationships).\n",
        "\n",
        "3Ô∏è‚É£ Works for Classification & Regression\n",
        "\n",
        "Can be used for both categorical and numerical predictions.\n",
        "\n",
        "4Ô∏è‚É£ Adapts to New Data\n",
        "\n",
        "Since KNN stores all data points, it automatically adjusts when new data is added.\n",
        "\n",
        "5Ô∏è‚É£ Interpretable & Explainable\n",
        "\n",
        "Easily understandable as it simply chooses the nearest K neighbors to make decisions.\n",
        "\n",
        "\n",
        "‚ùå Disadvantages of KNN\n",
        "\n",
        "1Ô∏è‚É£ Computationally Expensive\n",
        "\n",
        "Slow for large datasets because it calculates the distance for every test point against all training points.\n",
        "\n",
        "2Ô∏è‚É£ Memory-Intensive\n",
        "\n",
        "Needs to store the entire dataset, requiring large memory for big datasets.\n",
        "\n",
        "3Ô∏è‚É£ Sensitive to Noisy Data & Outliers\n",
        "\n",
        "If outliers are present, they can distort predictions significantly.\n",
        "\n",
        "4Ô∏è‚É£ Choosing the Right K is Critical\n",
        "\n",
        "Small K ‚Üí More variance, sensitive to noise\n",
        "\n",
        "Large K ‚Üí More bias, less flexibility\n",
        "\n",
        "5Ô∏è‚É£ Curse of Dimensionality\n",
        "\n",
        "When data has too many features, distance calculations become less meaningful.\n",
        "\n",
        "üìå When to Use KNN?\n",
        "\n",
        "‚úÖ When the dataset is small to medium-sized\n",
        "\n",
        "Works best when the number of samples is not too large.\n",
        "\n",
        "‚úÖ When data is not linearly separable\n",
        "\n",
        "Since KNN is non-parametric, it can capture complex decision boundaries.\n",
        "\n",
        "‚úÖ For problems where explainability is important\n",
        "\n",
        "KNN is intuitive and easy to interpret.\n",
        "\n",
        "‚úÖ When real-time predictions are not required\n",
        "\n",
        "Since KNN is slow during inference, it‚Äôs best for batch predictions rather than real-time use.\n",
        "\n",
        "‚úÖ For recommendation systems, pattern recognition, anomaly detection\n",
        "\n",
        "Commonly used in image classification, medical diagnosis, and recommendation engines.\n",
        "\n",
        "üöÄ Key Takeaways\n",
        "\n",
        "Use KNN for small, well-structured datasets where prediction speed is not critical.\n",
        "Avoid KNN for high-dimensional, large-scale datasets due to computational inefficiency.\n",
        "Choosing K wisely improves performance ‚Äì try cross-validation to find the best K.\n",
        "\n",
        "üí° Summary: KNN is simple but powerful! Use it wisely for the right tasks. üî•"
      ],
      "metadata": {
        "id": "ok3pIwuuvrWe"
      }
    }
  ]
}